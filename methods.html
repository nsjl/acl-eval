<html><head><meta charset='UTF-8'><style>
body { font-family: Roboto, Arial, sans-serif; background-color: #f5f5f5; max-width: 800px; margin: auto; padding: 40px 20px; }
h1 { font-size: 28px; color: #673ab7; margin-bottom: 5px; }
h2.title { font-size: 20px; font-weight: 400; color: #333; margin-bottom: 30px; }
.card { background: white; border-radius: 8px; padding: 16px; margin-bottom: 12px; box-shadow: 0 1px 3px rgba(0,0,0,0.1); }
.user { border-left: 5px solid #2196f3; }
.assistant { border-left: 5px solid #e91e63; }
.image { text-align: center; margin: 20px 0; }
a.feedback { display: inline-block; background: #673ab7; color: white; padding: 10px 20px; border-radius: 5px; text-decoration: none; margin-top: 40px; font-weight: bold; }
a.feedback:hover { background: #5e35b1; }
</style></head><body>
<h1>ACL Bot Chat Transcript</h1>
<h2 class='title'>Research Practices and Methods</h2>
<div class='card user'><b>User:</b> Which pretrained models are frequently used in Machine Translation papers?</div>
<div class='card assistant'><b>Bot:</b> Here is a summary of the most frequently used pretrained models in Machine Translation according to the data:<br><br>1. **BERT** is the most commonly used model with 228 mentions. This indicates its popularity in various machine translation tasks.<br>2. **mBART** (including its various versions like mBART50 and mBART-50) is also frequently used, with mentions such as 83 for mBART alone.<br>3. **Moses**, an SMT toolkit, is referenced 79 times, showing its continued relevance.<br>4. **XLM-R** is mentioned 73 times, indicating its strong presence in multilingual translation tasks.<br>5. **GPT-4** and its iterations like GPT-3.5 are not far behind, showing significant use with 57 mentions.<br>6. **Google Translate** is referenced 51 times, demonstrating its use as a benchmark or comparative tool.<br>7. **word2vec** and variants like word2Vec have 51 mentions, indicating its ongoing utility in NLP tasks.<br>8. **mBERT** and versions like multilingual BERT collectively received numerous references, similar to mBART variations.<br>9. **fastText** and **GloVe** also appear frequently at 36 and 35 mentions respectively.<br><br>Other noteworthy models include **mT5**, **NLLB** (No Language Left Behind), **Marian**, and **OpusMT**, each with several mentions highlighting their usage in machine translation efforts.<br><br>Overall, the list underscores the diversity of models used in Machine Translation, from older statistical models like Moses and GIZA++ to advanced neural models like BERT, mBART, and GPT series. This indicates a wide range of methodologies and technologies that researchers rely on for translation tasks.</div>
<div class='card user'><b>User:</b> Are there any papers that use Expectation-Maximization and report BLEU scores?</div>
<div class='card assistant'><b>Bot:</b> Here is a summary of the papers that use Expectation-Maximization and report BLEU scores:<br><br>1. **Title**: &quot;Fantastic Expressions and Where to Find Them: Chinese Simile Generation with Multiple Constraints&quot;  <br>   **Year**: 2023  <br>   **BLEU Score**: 0.197<br><br>2. **Title**: &quot;Data-Driven Sentence Simplification: Survey and Benchmark&quot;  <br>   **Year**: 2020  <br>   **BLEU Scores**: 0.5394, 0.7329<br><br>3. **Title**: &quot;A Comparative Analysis of the Effectiveness of Rare Tokens on Creative Expression using ramBERT&quot;  <br>   **Year**: 2023  <br>   **BLEU Score**: 0.123<br><br>4. **Title**: &quot;Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection&quot;  <br>   **Year**: 2023  <br>   **BLEU Score**: 0.28<br><br>5. **Title**: &quot;APPDIA: A Discourse-aware Transformer-based Style Transfer Model for Offensive Social Media Conversations&quot;  <br>   **Year**: 2022  <br>   **BLEU Score**: 0.496<br><br>6. **Title**: &quot;CoQAR: Question Rewriting on CoQA&quot;  <br>   **Year**: 2022  <br>   **BLEU Scores**: 0.737, 0.39<br><br>7. **Title**: &quot;JDDC 2.1: A Multimodal Chinese Dialogue Dataset with Joint Tasks of Query Rewriting, Response Generation, Discourse Parsing, and Summarization&quot;  <br>   **Year**: 2022  <br>   **BLEU Scores**: 0.1021, 0.0545<br><br>8. **Title**: &quot;Harmless Transfer Learning for Item Embeddings&quot;  <br>   **Year**: 2022  <br>   **BLEU Score**: 0.286<br><br>9. **Title**: &quot;On-the-fly Cross-lingual Masking for Multilingual Pre-training&quot;  <br>   **Year**: 2023  <br>   **BLEU Scores**: 0.359, 0.281, 0.344<br><br>10. **Title**: &quot;Simple TICO-19: A Dataset for Joint Translation and Simplification of COVID-19 Texts&quot;  <br>    **Year**: 2022  <br>    **BLEU Scores**: 0.5142, 0.5163, 0.5541, 0.5216, 0.3998, 0.5142, 0.3857, 0.3709<br><br>11. **Title**: &quot;Translation Memories as Baselines for Low-Resource Machine Translation&quot;  <br>    **Year**: 2022  <br>    **BLEU Scores**: 0.223, 0.36, 0.301, 0.376, 0.798, 0.509, 0.25, 0.25, 0.196<br><br>12. **Title**: &quot;INK: Injecting kNN Knowledge in Nearest Neighbor Machine Translation&quot;  <br>    **Year**: 2023  <br>    **BLEU Scores**: 0.5775, 0.619, 0.4912, 0.2306<br><br>13. **Title**: &quot;Dual-Feedback Knowledge Retrieval for Task-Oriented Dialogue Systems&quot;  <br>    **Year**: 2023  <br>    **BLEU Scores**: 0.1848, 0.251, 0.26<br><br>14. **Title**: &quot;Unsupervised End-to-End Task-Oriented Dialogue with LLMs: The Power of the Noisy Channel&quot;  <br>    **Year**: 2024  <br>    **BLEU Score**: 0.137<br><br>15. **Title**: &quot;Text Style Transferring via Adversarial Masking and Styled Filling&quot;  <br>    **Year**: 2022  <br>    **BLEU Scores**: 0.7667, 0.8155<br><br>These results indicate a variety of papers that employ the Expectation-Maximization algorithm for different tasks across multiple years, reporting BLEU scores ranging from around 0.0545 to 0.8155, highlighting varying degrees of efficacy and application.</div>
<div class='card user'><b>User:</b> What are the most commonly reported metrics for Question Answering in recent years?</div>
<div class='card assistant'><b>Bot:</b> Here are the most commonly reported metrics for Question Answering based on the data available:<br><br>1. **Accuracy**: Reported 2,305 times, making it the most frequently used metric for evaluating Question Answering systems. This metric measures the proportion of correct predictions out of all predictions made.<br><br>2. **F1-score**: Used 1,525 times, the F1-score combines precision and recall and is especially useful for imbalanced datasets.<br><br>3. **Exact Match (EM)**: Reported 617 times, this measures the percentage of predictions that match the ground truth exactly, without variation.<br><br>4. **F1 (Alternate name for F1-score)**: Mentioned 348 times, reflecting variations in naming conventions but still referring to the precision-recall balance.<br><br>5. **ROUGE-L**: Used 253 times, a variant of the ROUGE metric focused on longest common subsequences to evaluate coherence, particularly useful in summarization tasks but also applied in Question Answering.<br><br>6. **BLEU**: Reported 180 times, this metric is typically used for evaluating machine translation but also finds application in evaluating generated text in Question Answering contexts.<br><br>7. **ROUGE-1**: Mentioned 149 times, which evaluates the overlap of unigrams between the prediction and the ground truth.<br><br>8. **ROUGE-2**: Reported 102 times, similar to ROUGE-1 but focusing on bigrams for slightly more context-aware evaluation.<br><br>9. **F1 Score** (a case variation for F1-score): Used 92 times, noted separately possibly due to naming inconsistency.<br><br>10. **EM (Alternate name for Exact Match)**: Reported 67 times, another variation in terminology but referring to the same metric as Exact Match.<br><br>These metrics reflect the diverse ways in which Question Answering systems are evaluated, with a strong emphasis on both precise and holistic assessment approaches.</div>
<div class='card user'><b>User:</b> Can you summarize the trend between theoretical and engineering contributions over the past 10 years?</div>
<div class='card assistant'><b>Bot:</b> Here is a summary of the trend in NLP engineering experiments over the past decade:<br><br>- **2013**: There were 2,452 papers classified as &quot;NLP engineering experiment&quot;.<br>- **2014**: Slight increase to 2,506 papers.<br>- **2015**: A continued upward trend with 2,628 papers.<br>- **2016**: A significant rise to 3,625 papers.<br>- **2017**: A decrease to 2,904 papers.<br>- **2018**: A major increase to 4,100 papers.<br>- **2019**: Continued growth reaching 4,482 papers.<br>- **2020**: Another significant jump to 6,311 papers.<br>- **2021**: A slight decrease to 6,193 papers.<br>- **2022**: A further increase to 7,848 papers.<br>- **2023**: Again, an increase to 8,257 papers.<br>- **2024**: A projected sharp rise to 10,767 papers, indicating substantial growth.<br>- **2025**: A decrease to 1,146, likely reflecting data cutoff or entry errors.<br><br>The data shows a clear and consistent growth trend in the number of &quot;NLP engineering experiment&quot; papers over the past decade, with some fluctuations. Notably, there was a substantial increase from 2016 onwards, likely due to advancements and growing interest in machine learning and artificial intelligence in NLP. The dip in 2025 could be due to incomplete data.<br><br>This trend reflects the field&#x27;s shift towards practical implementations and experimentation in NLP technologies, coinciding with the increased availability of computational resources and large datasets.</div>
<div class='card user'><b>User:</b> Can you summarize the trend between theoretical and engineering contributions over the past 10 years?</div>
<div class='card assistant'><b>Bot:</b> Certainly! Here&#x27;s a summary of the trends in theoretical versus engineering contributions to the ACL Anthology over the past 10 years:<br><br>1. **NLP Engineering Experiment Contributions:**<br>   - There has been a steady and significant increase over the years in the number of NLP engineering experiment papers.<br>   - From 2013 (2,452 papers) to 2024 (10,767 papers), these contributions have seen a notable rise.<br>   - There seems to be a particular surge post-2019 where the numbers jump from 4,482 in 2019 to 6,311 in 2020.<br>   - The trend continues to maintain upward momentum, peaking in 2024.<br><br>2. **Theoretical Contributions:**<br>   - Compared to engineering contributions, theoretical papers are fewer in number.<br>   - There is some fluctuation, but generally, the numbers increase over time from 97 papers in 2013 to 149 in 2024.<br>   - The increase is not as steep as with engineering papers, though there is consistent growth.<br><br>3. **Comparative Insights:**<br>   - Throughout the past decade, NLP engineering experiments have greatly outnumbered theoretical contributions in quantity.<br>   - While both types of contributions are increasing, engineering papers exhibit a much more pronounced growth, particularly from 2019 onward.<br>   - This may suggest a trend in the research community towards more application-driven work, likely reflecting the technological advancements and increased emphasis on immediate practical applications in NLP.<br><br>In summary, while both types of contributions have grown in the ACL Anthology, the field of NLP appears to be leaning more heavily towards engineering experiments over the past decade, possibly driven by the surge in practical deployments and commercial interests.</div>
<div class='card user'><b>User:</b> Which pretrained models are most frequently used in Machine Translation papers?</div>
<div class='card assistant'><b>Bot:</b> Based on the query results, the following pretrained models are most frequently used in Machine Translation papers:<br><br>1. **BERT**: Utilized 228 times as a pretrained model in machine translation papers.<br>2. **mBART**: Used 83 times, known for its ability to handle multilingual tasks.<br>3. **Moses**: Mentioned 79 times, often used for statistical machine translation baselines.<br>4. **XLM-R**: Used 73 times, popular for cross-lingual tasks.<br>5. **GPT-4**: Cited 57 times, a prominent model in recent advances in generative NLP tasks.<br>6. **Google Translate**: Used 51 times, likely referenced for its translation outputs.<br>7. **word2vec**: Also mentioned 51 times, used for obtaining vector representations of words.<br>8. **mBERT**: Cited 48 times, a multilingual variant of BERT.<br>9. **XLM**: Used 42 times, another model developed for cross-lingual understanding.<br>10. **GIZA++**: Used 40 times, often employed in statistical alignment tasks.<br><br>These models are frequently referenced and utilized due to their capabilities in processing multilingual text and providing baselines or enhancements for translation models. This reflects a mix of classical systems like Moses and modern transformer-based approaches like BERT and GPT-4.</div>
<div class='card user'><b>User:</b> Which pretrained models are most frequently used in Machine Translation papers?</div>
<div class='card assistant'><b>Bot:</b> Based on the query results, here is a summary of the most frequently used pretrained models in Machine Translation papers:<br><br>1. **BERT** - Mentioned 228 times<br>2. **mBART** - Mentioned 83 times<br>3. **Moses** - Mentioned 79 times<br>4. **XLM-R** - Mentioned 73 times<br>5. **GPT-4** - Mentioned 57 times<br>6. **Google Translate** - Mentioned 51 times<br>7. **word2vec** - Mentioned 51 times<br>8. **mBERT** - Mentioned 48 times<br>9. **XLM** - Mentioned 42 times<br>10. **GIZA++** - Mentioned 40 times<br>11. **fastText** - Mentioned 36 times<br>12. **GloVe** - Mentioned 35 times<br>13. **LaBSE** - Mentioned 34 times<br>14. **XLM-RoBERTa** - Mentioned 31 times<br>15. **BART** - Mentioned 30 times<br>16. **ChatGPT** - Mentioned 30 times<br>17. **mT5** - Mentioned 27 times<br>18. **RoBERTa** - Mentioned 26 times<br>19. **LASER** - Mentioned 26 times<br>20. **mBART50** - Mentioned 25 times<br><br>These pretrained models are widely used in the Machine Translation domain for their effectiveness in handling multilingual text processing and enhancing translation tasks. They range from models like BERT and GPT-4, known for their general NLP capabilities, to specialized translation and multilingual approaches like mBART and XLM-R.</div>
<div class='card user'><b>User:</b> What are the most commonly reported metrics for Question Answering in recent years?</div>
<div class='card assistant'><b>Bot:</b> From the query results, the most commonly reported metrics for Question Answering in recent years are:<br><br>1. **Accuracy** - Reported 2305 times, a measure of the percentage of correct answers.<br>2. **F1-score** - Reported 1525 times, a harmonic mean of precision and recall, balancing both false positives and false negatives.<br>3. **Exact Match** - Reported 617 times, evaluates the percentage of predictions that match exactly with the ground truth.<br>4. **F1** - Reported 348 times, similar to F1-score, focusing on the balance between precision and recall.<br>5. **ROUGE-L** - Reported 253 times, measures the overlap of the longest common subsequence between the predicted and reference sentences, often used for evaluating summarization and translation tasks.<br>6. **BLEU** - Reported 180 times, a metric for evaluating the quality of text that has been machine-translated from one natural language to another.<br>7. **ROUGE-1** - Reported 149 times, considers the overlap of unigrams between the predicted and reference texts.<br>8. **ROUGE-2** - Reported 102 times, measures the overlap of bigrams.<br>9. **F1 Score** - Reported 92 times, another variant of the F1 metric.<br>10. **EM (Exact Match)** - Reported 67 times, another variant that strictly assesses exact answer matches.<br><br>These metrics reflect a broad focus on both precision and recall in question answering tasks, which tend to be nuanced and involve a mix of generative and extractive components. They highlight the importance of evaluating not just the correctness but also the completeness and appropriateness of the task outputs.</div>
<p><a class='feedback' href='https://docs.google.com/forms/d/e/1FAIpQLSc8a44X_4Q7rb5bRXBpomUM_D3vr0fC9nCNY40P7aIzXHX13g/viewform?usp=dialog' target='_blank'>Click here to provide feedback</a></p>
</body></html>